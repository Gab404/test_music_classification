{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69249fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch, librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import random\n",
    "import opendatasets as od\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193fee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(path):\n",
    "    img = Image.open(path)\n",
    "    img_cropped = img.crop(box = (54, 35, 390, 253))\n",
    "    img_resized = img_cropped.resize((round(img_cropped.size[0]*0.5), round(img_cropped.size[1]*0.5)))\n",
    "    img_resized.save(path)\n",
    "    img.close()\n",
    "    img_cropped.close()\n",
    "    img_resized.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f04de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_to_jpg(path, name, music_type):\n",
    "    # Convert one file .wav to .jpg\n",
    "    \n",
    "    if not os.path.exists(path):\n",
    "        print('Music file not found')\n",
    "        return -1\n",
    "    if os.path.exists(f\"images_sound/{music_type}/{name[:-4]}.jpg\"):\n",
    "        return 0\n",
    "    y, sr = librosa.load(path)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000, power=3.9)\n",
    "    fig, ax = plt.subplots()\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    img = librosa.display.specshow(S_db, sr=sr, fmax=8000, ax=ax)\n",
    "    if not os.path.exists(f\"images_sound/{music_type}\"):\n",
    "        os.mkdir(f\"images_sound/{music_type}\")\n",
    "    plt.savefig(f\"images_sound/{music_type}/{name[:-4]}.jpg\")\n",
    "    plt.close('all')\n",
    "    resize_img(f\"images_sound/{music_type}/{name[:-4]}.jpg\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73302e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_music(path_data):\n",
    "    # Convert all file .wav to .jpg\n",
    "    # Risk of crashing\n",
    "    \n",
    "    label_dict = {}\n",
    "    index_label = 0\n",
    "\n",
    "    if not os.path.exists(path_data):\n",
    "        print('Data folder not found')\n",
    "        return -1\n",
    "    if not os.path.exists(\"images_sound\"):\n",
    "        os.mkdir(\"images_sound\")\n",
    "\n",
    "    for music_type in os.listdir(path_data):\n",
    "        d = os.path.join(path_data, music_type)\n",
    "        if os.path.isdir(d):\n",
    "            label_dict[music_type] = index_label\n",
    "            index_label += 1\n",
    "            for music in os.listdir(d):\n",
    "                entire_path = os.path.join(d, music)\n",
    "                wav_to_jpg(entire_path, music, music_type)\n",
    "\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "882e3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_tensor(entire_path):\n",
    "    img = Image.open(entire_path)\n",
    "    resize = transforms.functional.resize(img, size=[200, 200])\n",
    "    transform = transforms.Compose([transforms.Normalize([0.5], [0.5])])\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    new_img = transform(to_tensor(resize))\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd014231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_tensor(path_data, batch_size, label_dict):\n",
    "    my_tensor = []\n",
    "    all_label = []\n",
    "    i = 0\n",
    "\n",
    "    for k in range(0, 1000):\n",
    "        my_tensor.append(torch.empty(3, 200, 200))\n",
    "    for music_type in os.listdir(path_data):\n",
    "        d = os.path.join(path_data, music_type)\n",
    "        if os.path.isdir(d):\n",
    "            for music in os.listdir(d):\n",
    "                entire_path = os.path.join(d, music)\n",
    "                my_tensor[i] = img_to_tensor(entire_path)\n",
    "                all_label.append(label_dict[music_type])\n",
    "                i += 1\n",
    "    return my_tensor, all_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a48b38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_2_list(first_list, second_list):\n",
    "    # Shuffle with the same way the list of tensor\n",
    "    zip_list = list(zip(first_list, second_list))\n",
    "    random.shuffle(zip_list)\n",
    "    first_list, second_list = zip(*zip_list)\n",
    "    return first_list, second_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa8d0ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping, found downloaded files in \"./gtzan-dataset-music-genre-classification\" (use force=True to force download)\n"
     ]
    }
   ],
   "source": [
    "# username: gabriel404guietdupre\n",
    "# key: 97d328e7f389ab6055fc7b9dd41327ea\n",
    "\n",
    "dataset = \"https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification\"\n",
    "od.download(dataset)\n",
    "if os.path.exists(\"gtzan-dataset-music-genre-classification/Data/genres_original/jazz/jazz.00054.wav\"):\n",
    "    os.remove('gtzan-dataset-music-genre-classification/Data/genres_original/jazz/jazz.00054.wav')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f4250f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "lr = 0.01\n",
    "\n",
    "# Convert all music to image and create a dict of all song's type\n",
    "label_dict = convert_all_music('gtzan-dataset-music-genre-classification/Data/genres_original')\n",
    "\n",
    "# Create 2 tensors, my_tensor with all images in tensor with a batch size, and all_label of each image in order\n",
    "my_tensor, all_label = create_all_tensor('images_sound', batch_size, label_dict)\n",
    "my_tensor, all_label = shuffle_2_list(my_tensor, all_label)\n",
    "\n",
    "train_loader = []\n",
    "train_label = []\n",
    "test_loader = []\n",
    "test_label = []\n",
    "j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c7c18f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "i = 0\n",
    "list_batch_tensor = []\n",
    "list_batch_label = []\n",
    "for k in range (0, int(len(my_tensor) / batch_size)):\n",
    "    list_batch_tensor.append(torch.empty(batch_size, 3, 200, 200))\n",
    "    list_batch_label.append(torch.empty(batch_size, dtype=torch.long))\n",
    "for l, tensor in enumerate(my_tensor):\n",
    "    if (i >= int(len(my_tensor) / batch_size)):\n",
    "        break\n",
    "    list_batch_tensor[i][j] = my_tensor[l]\n",
    "    list_batch_label[i][j] = all_label[l]\n",
    "    if (j == batch_size - 1):\n",
    "        j = 0\n",
    "        i += 1\n",
    "    else:\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dd5ce252",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in range (len(list_batch_tensor)):\n",
    "    if i >= (80 * len(list_batch_tensor)) / 100:\n",
    "        test_loader.append(torch.empty(batch_size, 3, 200, 200))\n",
    "        test_loader[j] = list_batch_tensor[i]\n",
    "        test_label.append(torch.empty(batch_size, dtype=torch.long))\n",
    "        test_label[j] = list_batch_label[i]\n",
    "        j += 1\n",
    "    else:\n",
    "        train_loader.append(torch.empty(batch_size, 3, 200, 200))\n",
    "        train_loader[i] = list_batch_tensor[i]\n",
    "        train_label.append(torch.empty(batch_size, dtype=torch.long))\n",
    "        train_label[i] = list_batch_label[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "140108c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"Intitalize neural net layers\"\"\"\n",
    "    super(Net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv6 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=0)\n",
    "    self.fc1 = nn.Linear(in_features=6400, out_features=2000)\n",
    "    self.fc2 = nn.Linear(in_features=2000, out_features=500)\n",
    "    self.fc3 = nn.Linear(in_features=500, out_features=100)\n",
    "    self.fc4 = nn.Linear(in_features=100, out_features=10)\n",
    "\n",
    "    self.batchnorm1 = nn.BatchNorm2d(num_features=8)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(num_features=16)\n",
    "    self.batchnorm3 = nn.BatchNorm2d(num_features=32)\n",
    "    self.batchnorm4 = nn.BatchNorm2d(num_features=64)\n",
    "    self.batchnorm5 = nn.BatchNorm2d(num_features=128)\n",
    "    self.batchnorm6 = nn.BatchNorm2d(num_features=256)\n",
    "\n",
    "    self.dropout = nn.Dropout(p=0.3, inplace=False)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Conv layer 1.\n",
    "    x = self.conv1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.batchnorm1(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 2.\n",
    "    x = self.conv2(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.batchnorm2(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 3.\n",
    "    x = self.conv3(x)\n",
    "    x = self.batchnorm3(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 4.\n",
    "    x = self.conv4(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.batchnorm4(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Fully connected layer 1.\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.dropout(x)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    x = self.fc4(x)\n",
    "    #x = F.softmax(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4b7df61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 20, loss = 2.2785\n",
      "epoch 2 / 20, loss = 2.2128\n",
      "epoch 3 / 20, loss = 2.1528\n",
      "epoch 4 / 20, loss = 2.0729\n",
      "epoch 5 / 20, loss = 1.9231\n",
      "epoch 6 / 20, loss = 1.8157\n",
      "epoch 7 / 20, loss = 1.7522\n",
      "epoch 8 / 20, loss = 1.5939\n",
      "epoch 9 / 20, loss = 1.4755\n",
      "epoch 10 / 20, loss = 1.3298\n",
      "epoch 11 / 20, loss = 1.1547\n",
      "epoch 12 / 20, loss = 1.0483\n",
      "epoch 13 / 20, loss = 0.8310\n",
      "epoch 14 / 20, loss = 0.7403\n",
      "epoch 15 / 20, loss = 0.6751\n",
      "epoch 16 / 20, loss = 0.6383\n",
      "epoch 17 / 20, loss = 0.4145\n",
      "epoch 18 / 20, loss = 0.3040\n",
      "epoch 19 / 20, loss = 0.2555\n",
      "epoch 20 / 20, loss = 0.1920\n",
      "End of training\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "#training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        #forward\n",
    "        outputs = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, train_label[i])\n",
    "        #backwards\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch {epoch + 1} / {num_epochs}, loss = {loss.item():.4f}')\n",
    "print(\"End of training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa67c9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 192 test images: 57.291666666666664 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for i, images in enumerate(test_loader):\n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += test_label[i].size(0)\n",
    "        n_correct += (predicted == test_label[i]).sum().item()\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            label = test_label[i][j]\n",
    "            pred = predicted[j]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "            \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the {n_samples} test images: {acc} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12149907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
