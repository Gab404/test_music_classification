{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69249fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch, librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from data_preprocessing import convert_all_music, create_all_tensor\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "193fee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_img(path):\n",
    "    img = Image.open(path)\n",
    "    img_cropped = img.crop(box = (54, 35, 390, 253))\n",
    "    img_cropped.save(path)\n",
    "    img.close()\n",
    "    img_cropped.close()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f04de18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wav_to_png(path, name, music_type):\n",
    "    if not os.path.exists(path):\n",
    "        print('Music file not found')\n",
    "        return -1\n",
    "    if os.path.exists(f\"images_sound/{music_type}/{name[:-4]}.jpg\"):\n",
    "        return 0\n",
    "    y, sr = librosa.load(path)\n",
    "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000, power=3.9)\n",
    "    fig, ax = plt.subplots()\n",
    "    S_db = librosa.power_to_db(S, ref=np.max)\n",
    "    img = librosa.display.specshow(S_db, sr=sr, fmax=8000, ax=ax)\n",
    "    if not os.path.exists(f\"images_sound/{music_type}\"):\n",
    "        os.mkdir(f\"images_sound/{music_type}\")\n",
    "    plt.savefig(f\"images_sound/{music_type}/{name[:-4]}.jpg\")\n",
    "    plt.close('all')\n",
    "    resize_img(f\"images_sound/{music_type}/{name[:-4]}.jpg\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73302e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_all_music(path_data):\n",
    "    label_dict = {}\n",
    "    index_label = 0\n",
    "\n",
    "    if not os.path.exists(path_data):\n",
    "        print('Data folder not found')\n",
    "        return -1\n",
    "    if not os.path.exists(\"images_sound\"):\n",
    "        os.mkdir(\"images_sound\")\n",
    "\n",
    "    for music_type in os.listdir(path_data):\n",
    "        d = os.path.join(path_data, music_type)\n",
    "        if os.path.isdir(d):\n",
    "            label_dict[music_type] = index_label\n",
    "            index_label += 1\n",
    "            for music in os.listdir(d):\n",
    "                entire_path = os.path.join(d, music)\n",
    "                wav_to_png(entire_path, music, music_type)\n",
    "\n",
    "    return label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "882e3060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_tensor(entire_path):\n",
    "    img = Image.open(entire_path)\n",
    "    resize = transforms.functional.resize(img, size=[200, 200])\n",
    "    transform = transforms.Compose([transforms.Normalize([0.5], [0.5])])\n",
    "    to_tensor = transforms.ToTensor()\n",
    "    new_img = transform(to_tensor(resize))\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd014231",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_all_tensor(path_data, batch_size, label_dict):\n",
    "    my_tensor = []\n",
    "    all_label = []\n",
    "    i = 0\n",
    "\n",
    "    for k in range(0, 1000):\n",
    "        my_tensor.append(torch.empty(3, 200, 200))\n",
    "    for music_type in os.listdir(path_data):\n",
    "        d = os.path.join(path_data, music_type)\n",
    "        if os.path.isdir(d):\n",
    "            for music in os.listdir(d):\n",
    "                entire_path = os.path.join(d, music)\n",
    "                my_tensor[i] = img_to_tensor(entire_path)\n",
    "                all_label.append(label_dict[music_type])\n",
    "                i += 1\n",
    "    return my_tensor, all_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a48b38df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_2_list(first_list, second_list):\n",
    "    # Shuffle with the same way the list of tensor\n",
    "    zip_list = list(zip(first_list, second_list))\n",
    "    random.shuffle(zip_list)\n",
    "    first_list, second_list = zip(*zip_list)\n",
    "    return first_list, second_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f4250f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 20\n",
    "lr = 0.01\n",
    "\n",
    "# Convert all music to image and create a dict of all song's type\n",
    "label_dict = convert_all_music('Data/genres_original')\n",
    "\n",
    "# Create 2 tensors, my_tensor with all images in tensor with a batch size, and all_label of each image in order\n",
    "my_tensor, all_label = create_all_tensor('images_sound', batch_size, label_dict)\n",
    "my_tensor, all_label = shuffle_2_list(my_tensor, all_label)\n",
    "\n",
    "train_loader = []\n",
    "train_label = []\n",
    "test_loader = []\n",
    "test_label = []\n",
    "j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c7c18f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "i = 0\n",
    "list_batch_tensor = []\n",
    "list_batch_label = []\n",
    "for k in range (0, int(len(my_tensor) / batch_size)):\n",
    "    list_batch_tensor.append(torch.empty(batch_size, 3, 200, 200))\n",
    "    list_batch_label.append(torch.empty(batch_size, dtype=torch.long))\n",
    "for l, tensor in enumerate(my_tensor):\n",
    "    if (i >= int(len(my_tensor) / batch_size)):\n",
    "        break\n",
    "    list_batch_tensor[i][j] = my_tensor[l]\n",
    "    list_batch_label[i][j] = all_label[l]\n",
    "    if (j == batch_size - 1):\n",
    "        j = 0\n",
    "        i += 1\n",
    "    else:\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd5ce252",
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 0\n",
    "for i in range (len(list_batch_tensor)):\n",
    "    if i >= (80 * len(list_batch_tensor)) / 100:\n",
    "        test_loader.append(torch.empty(batch_size, 3, 200, 200))\n",
    "        test_loader[j] = list_batch_tensor[i]\n",
    "        test_label.append(torch.empty(batch_size, dtype=torch.long))\n",
    "        test_label[j] = list_batch_label[i]\n",
    "        j += 1\n",
    "    else:\n",
    "        train_loader.append(torch.empty(batch_size, 3, 200, 200))\n",
    "        train_loader[i] = list_batch_tensor[i]\n",
    "        train_label.append(torch.empty(batch_size, dtype=torch.long))\n",
    "        train_label[i] = list_batch_label[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "140108c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "  def __init__(self):\n",
    "    \"\"\"Intitalize neural net layers\"\"\"\n",
    "    super(Net, self).__init__()\n",
    "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv2 = nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv5 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "    self.conv6 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=0)\n",
    "    self.fc1 = nn.Linear(in_features=256, out_features=128)\n",
    "    self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "    self.fc3 = nn.Linear(in_features=64, out_features=32)\n",
    "    self.fc4 = nn.Linear(in_features=32, out_features=10)\n",
    "\n",
    "    self.batchnorm1 = nn.BatchNorm2d(num_features=8)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(num_features=16)\n",
    "    self.batchnorm3 = nn.BatchNorm2d(num_features=32)\n",
    "    self.batchnorm4 = nn.BatchNorm2d(num_features=64)\n",
    "    self.batchnorm5 = nn.BatchNorm2d(num_features=128)\n",
    "    self.batchnorm6 = nn.BatchNorm2d(num_features=256)\n",
    "\n",
    "    self.dropout = nn.Dropout(p=0.3, inplace=False)\n",
    "\n",
    "  def forward(self, x):\n",
    "    # Conv layer 1.\n",
    "    x = self.conv1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.batchnorm1(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 2.\n",
    "    x = self.conv2(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.batchnorm2(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 3.\n",
    "    x = self.conv3(x)\n",
    "    x = self.batchnorm3(x)\n",
    "    x = F.relu(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Conv layer 4.\n",
    "    x = self.conv4(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.batchnorm4(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    \n",
    "    # Conv layer 5.\n",
    "    x = self.conv5(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.batchnorm5(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "    \n",
    "    # Conv layer 6.\n",
    "    x = self.conv6(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.batchnorm6(x)\n",
    "    x = F.max_pool2d(x, kernel_size=2)\n",
    "\n",
    "    # Fully connected layer 1.\n",
    "    x = torch.flatten(x, 1)\n",
    "    x = self.dropout(x)\n",
    "    x = F.relu(self.fc1(x))\n",
    "    x = F.relu(self.fc2(x))\n",
    "    x = F.relu(self.fc3(x))\n",
    "    x = self.fc4(x)\n",
    "    x = F.softmax(x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f4b7df61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 20, loss = 2.2571\n",
      "epoch 2 / 20, loss = 2.1695\n",
      "epoch 3 / 20, loss = 2.0289\n",
      "epoch 4 / 20, loss = 1.9071\n",
      "epoch 5 / 20, loss = 1.8030\n",
      "epoch 6 / 20, loss = 1.7374\n",
      "epoch 7 / 20, loss = 1.6499\n",
      "epoch 8 / 20, loss = 1.5490\n",
      "epoch 9 / 20, loss = 1.4285\n",
      "epoch 10 / 20, loss = 1.3179\n",
      "epoch 11 / 20, loss = 1.1494\n",
      "epoch 12 / 20, loss = 1.0589\n",
      "epoch 13 / 20, loss = 1.0186\n",
      "epoch 14 / 20, loss = 0.8341\n",
      "epoch 15 / 20, loss = 0.7687\n",
      "epoch 16 / 20, loss = 0.5601\n",
      "epoch 17 / 20, loss = 0.4684\n",
      "epoch 18 / 20, loss = 0.3741\n",
      "epoch 19 / 20, loss = 0.2874\n",
      "epoch 20 / 20, loss = 0.1764\n",
      "End of training\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "#training loop\n",
    "for epoch in range(20):\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        #forward\n",
    "        outputs = model(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, train_label[i])\n",
    "        #backwards\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'epoch {epoch + 1} / {num_epochs}, loss = {loss.item():.4f}')\n",
    "print(\"End of training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa67c9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 192 test images: 60.416666666666664 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for i, images in enumerate(test_loader):\n",
    "        outputs = model(images)\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        n_samples += test_label[i].size(0)\n",
    "        n_correct += (predicted == test_label[i]).sum().item()\n",
    "\n",
    "        for j in range(batch_size):\n",
    "            label = test_label[i][j]\n",
    "            pred = predicted[j]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "            \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network on the {n_samples} test images: {acc} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21377089",
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(\"./exemple_country.wav\")\n",
    "S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128, fmax=8000, power=3.9)\n",
    "fig, ax = plt.subplots()\n",
    "S_db = librosa.power_to_db(S, ref=np.max)\n",
    "img = librosa.display.specshow(S_db, sr=sr, fmax=8000, ax=ax)\n",
    "plt.savefig(f\"test.jpg\")\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a66c753e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"./test.jpg\")\n",
    "resize = transforms.functional.resize(img, size=[200, 200])\n",
    "transform = transforms.Compose([transforms.Normalize([0.5], [0.5])])\n",
    "to_tensor = transforms.ToTensor()\n",
    "new_img = transform(to_tensor(resize))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "49775843",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor = torch.empty(batch_size, 3, 200, 200)\n",
    "for i in range(0, batch_size):\n",
    "    tensor[i] = new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4a30d7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 4, 2, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(tensor)\n",
    "        \n",
    "_, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "14138b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La musique est pop\n"
     ]
    }
   ],
   "source": [
    "for music_type in label_dict:\n",
    "    if label_dict[music_type] == 2:\n",
    "        print(\"La musique est\", music_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3cf510",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36b652c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
